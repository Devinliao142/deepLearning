{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1671)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 28, 28), (200000,))\n",
      "('Validation set', (10000, 28, 28), (10000,))\n",
      "('Test set', (10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/home/peizhou.liao/deep_learning/Google/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (200000, 784), (200000, 10))\n",
      "('Validation set', (10000, 784), (10000, 10))\n",
      "('Test set', (10000, 784), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSES = 10\n",
    "RESHAPED = 784\n",
    "OPTIMIZER = SGD()\n",
    "BATCH_SIZE = 256\n",
    "NB_EPOCH = 100\n",
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.9795 - acc: 0.7440 - val_loss: 0.7853 - val_acc: 0.7951\n",
      "Epoch 2/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.7436 - acc: 0.8052 - val_loss: 0.7341 - val_acc: 0.8068\n",
      "Epoch 3/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.7103 - acc: 0.8125 - val_loss: 0.7126 - val_acc: 0.8129\n",
      "Epoch 4/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6935 - acc: 0.8167 - val_loss: 0.6999 - val_acc: 0.8163\n",
      "Epoch 5/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6827 - acc: 0.8194 - val_loss: 0.6910 - val_acc: 0.8175\n",
      "Epoch 6/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6750 - acc: 0.8214 - val_loss: 0.6845 - val_acc: 0.8208\n",
      "Epoch 7/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6691 - acc: 0.8232 - val_loss: 0.6794 - val_acc: 0.8216\n",
      "Epoch 8/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6644 - acc: 0.8245 - val_loss: 0.6754 - val_acc: 0.8217\n",
      "Epoch 9/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6605 - acc: 0.8255 - val_loss: 0.6722 - val_acc: 0.8233\n",
      "Epoch 10/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6572 - acc: 0.8265 - val_loss: 0.6689 - val_acc: 0.8246\n",
      "Epoch 11/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6545 - acc: 0.8272 - val_loss: 0.6670 - val_acc: 0.8235\n",
      "Epoch 12/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6521 - acc: 0.8277 - val_loss: 0.6649 - val_acc: 0.8239\n",
      "Epoch 13/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6500 - acc: 0.8282 - val_loss: 0.6629 - val_acc: 0.8244\n",
      "Epoch 14/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6481 - acc: 0.8288 - val_loss: 0.6614 - val_acc: 0.8245\n",
      "Epoch 15/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6465 - acc: 0.8291 - val_loss: 0.6600 - val_acc: 0.8247\n",
      "Epoch 16/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6450 - acc: 0.8295 - val_loss: 0.6587 - val_acc: 0.8249\n",
      "Epoch 17/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6437 - acc: 0.8298 - val_loss: 0.6579 - val_acc: 0.8246\n",
      "Epoch 18/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6425 - acc: 0.8301 - val_loss: 0.6568 - val_acc: 0.8247\n",
      "Epoch 19/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6413 - acc: 0.8303 - val_loss: 0.6557 - val_acc: 0.8256\n",
      "Epoch 20/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6403 - acc: 0.8306 - val_loss: 0.6549 - val_acc: 0.8262\n",
      "Epoch 21/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6394 - acc: 0.8307 - val_loss: 0.6541 - val_acc: 0.8263\n",
      "Epoch 22/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6385 - acc: 0.8309 - val_loss: 0.6537 - val_acc: 0.8255\n",
      "Epoch 23/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6377 - acc: 0.8314 - val_loss: 0.6528 - val_acc: 0.8260\n",
      "Epoch 24/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6370 - acc: 0.8315 - val_loss: 0.6520 - val_acc: 0.8258\n",
      "Epoch 25/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6363 - acc: 0.8318 - val_loss: 0.6513 - val_acc: 0.8255\n",
      "Epoch 26/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6356 - acc: 0.8317 - val_loss: 0.6510 - val_acc: 0.8276\n",
      "Epoch 27/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6350 - acc: 0.8322 - val_loss: 0.6505 - val_acc: 0.8268\n",
      "Epoch 28/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6344 - acc: 0.8324 - val_loss: 0.6500 - val_acc: 0.8266\n",
      "Epoch 29/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6338 - acc: 0.8324 - val_loss: 0.6499 - val_acc: 0.8278\n",
      "Epoch 30/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6333 - acc: 0.8328 - val_loss: 0.6491 - val_acc: 0.8279\n",
      "Epoch 31/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6328 - acc: 0.8330 - val_loss: 0.6490 - val_acc: 0.8264\n",
      "Epoch 32/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6324 - acc: 0.8329 - val_loss: 0.6484 - val_acc: 0.8271\n",
      "Epoch 33/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6319 - acc: 0.8331 - val_loss: 0.6481 - val_acc: 0.8276\n",
      "Epoch 34/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6314 - acc: 0.8332 - val_loss: 0.6478 - val_acc: 0.8283\n",
      "Epoch 35/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6311 - acc: 0.8334 - val_loss: 0.6474 - val_acc: 0.8278\n",
      "Epoch 36/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6307 - acc: 0.8333 - val_loss: 0.6470 - val_acc: 0.8288\n",
      "Epoch 37/100\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.6303 - acc: 0.8333 - val_loss: 0.6467 - val_acc: 0.8287\n",
      "Epoch 38/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6299 - acc: 0.8336 - val_loss: 0.6466 - val_acc: 0.8284\n",
      "Epoch 39/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6296 - acc: 0.8337 - val_loss: 0.6462 - val_acc: 0.8282\n",
      "Epoch 40/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6293 - acc: 0.8338 - val_loss: 0.6460 - val_acc: 0.8289\n",
      "Epoch 41/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6289 - acc: 0.8338 - val_loss: 0.6456 - val_acc: 0.8291\n",
      "Epoch 42/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6286 - acc: 0.8337 - val_loss: 0.6454 - val_acc: 0.8296\n",
      "Epoch 43/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6283 - acc: 0.8339 - val_loss: 0.6452 - val_acc: 0.8297\n",
      "Epoch 44/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6281 - acc: 0.8341 - val_loss: 0.6452 - val_acc: 0.8295\n",
      "Epoch 45/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6278 - acc: 0.8340 - val_loss: 0.6449 - val_acc: 0.8290\n",
      "Epoch 46/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6275 - acc: 0.8344 - val_loss: 0.6450 - val_acc: 0.8288\n",
      "Epoch 47/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6272 - acc: 0.8343 - val_loss: 0.6447 - val_acc: 0.8286\n",
      "Epoch 48/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6270 - acc: 0.8344 - val_loss: 0.6446 - val_acc: 0.8291\n",
      "Epoch 49/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6268 - acc: 0.8347 - val_loss: 0.6441 - val_acc: 0.8297\n",
      "Epoch 50/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6265 - acc: 0.8347 - val_loss: 0.6439 - val_acc: 0.8296\n",
      "Epoch 51/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6263 - acc: 0.8346 - val_loss: 0.6439 - val_acc: 0.8299\n",
      "Epoch 52/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6260 - acc: 0.8345 - val_loss: 0.6439 - val_acc: 0.8291\n",
      "Epoch 53/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6258 - acc: 0.8346 - val_loss: 0.6436 - val_acc: 0.8301\n",
      "Epoch 54/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6256 - acc: 0.8347 - val_loss: 0.6435 - val_acc: 0.8294\n",
      "Epoch 55/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6254 - acc: 0.8349 - val_loss: 0.6436 - val_acc: 0.8290\n",
      "Epoch 56/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6252 - acc: 0.8349 - val_loss: 0.6431 - val_acc: 0.8306\n",
      "Epoch 57/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6250 - acc: 0.8349 - val_loss: 0.6431 - val_acc: 0.8296\n",
      "Epoch 58/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6248 - acc: 0.8350 - val_loss: 0.6429 - val_acc: 0.8299\n",
      "Epoch 59/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6247 - acc: 0.8351 - val_loss: 0.6428 - val_acc: 0.8305\n",
      "Epoch 60/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6244 - acc: 0.8351 - val_loss: 0.6427 - val_acc: 0.8309\n",
      "Epoch 61/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6243 - acc: 0.8352 - val_loss: 0.6426 - val_acc: 0.8301\n",
      "Epoch 62/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6241 - acc: 0.8351 - val_loss: 0.6424 - val_acc: 0.8303\n",
      "Epoch 63/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6239 - acc: 0.8351 - val_loss: 0.6425 - val_acc: 0.8300\n",
      "Epoch 64/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6238 - acc: 0.8352 - val_loss: 0.6425 - val_acc: 0.8301\n",
      "Epoch 65/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6236 - acc: 0.8351 - val_loss: 0.6423 - val_acc: 0.8296\n",
      "Epoch 66/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6234 - acc: 0.8352 - val_loss: 0.6422 - val_acc: 0.8308\n",
      "Epoch 67/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6233 - acc: 0.8354 - val_loss: 0.6421 - val_acc: 0.8301\n",
      "Epoch 68/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6231 - acc: 0.8356 - val_loss: 0.6422 - val_acc: 0.8302\n",
      "Epoch 69/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6230 - acc: 0.8355 - val_loss: 0.6417 - val_acc: 0.8306\n",
      "Epoch 70/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6229 - acc: 0.8357 - val_loss: 0.6416 - val_acc: 0.8306\n",
      "Epoch 71/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6227 - acc: 0.8354 - val_loss: 0.6420 - val_acc: 0.8303\n",
      "Epoch 72/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6226 - acc: 0.8357 - val_loss: 0.6413 - val_acc: 0.8306\n",
      "Epoch 73/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6225 - acc: 0.8356 - val_loss: 0.6416 - val_acc: 0.8308\n",
      "Epoch 74/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6223 - acc: 0.8357 - val_loss: 0.6411 - val_acc: 0.8312\n",
      "Epoch 75/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6222 - acc: 0.8357 - val_loss: 0.6411 - val_acc: 0.8307\n",
      "Epoch 76/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6220 - acc: 0.8356 - val_loss: 0.6412 - val_acc: 0.8310\n",
      "Epoch 77/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6219 - acc: 0.8356 - val_loss: 0.6411 - val_acc: 0.8300\n",
      "Epoch 78/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6218 - acc: 0.8358 - val_loss: 0.6412 - val_acc: 0.8305\n",
      "Epoch 79/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6216 - acc: 0.8359 - val_loss: 0.6409 - val_acc: 0.8307\n",
      "Epoch 80/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6215 - acc: 0.8359 - val_loss: 0.6412 - val_acc: 0.8305\n",
      "Epoch 81/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6214 - acc: 0.8360 - val_loss: 0.6407 - val_acc: 0.8313\n",
      "Epoch 82/100\n",
      "200000/200000 [==============================] - 1s 7us/step - loss: 0.6213 - acc: 0.8360 - val_loss: 0.6406 - val_acc: 0.8309\n",
      "Epoch 83/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6212 - acc: 0.8359 - val_loss: 0.6408 - val_acc: 0.8299\n",
      "Epoch 84/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6211 - acc: 0.8359 - val_loss: 0.6406 - val_acc: 0.8311\n",
      "Epoch 85/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6210 - acc: 0.8359 - val_loss: 0.6408 - val_acc: 0.8303\n",
      "Epoch 86/100\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.6209 - acc: 0.8361 - val_loss: 0.6405 - val_acc: 0.8306\n",
      "Epoch 87/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6208 - acc: 0.8360 - val_loss: 0.6405 - val_acc: 0.8307\n",
      "Epoch 88/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6207 - acc: 0.8363 - val_loss: 0.6405 - val_acc: 0.8303\n",
      "Epoch 89/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6205 - acc: 0.8360 - val_loss: 0.6403 - val_acc: 0.8306\n",
      "Epoch 90/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6205 - acc: 0.8362 - val_loss: 0.6403 - val_acc: 0.8310\n",
      "Epoch 91/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6203 - acc: 0.8362 - val_loss: 0.6402 - val_acc: 0.8307\n",
      "Epoch 92/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6202 - acc: 0.8363 - val_loss: 0.6403 - val_acc: 0.8310\n",
      "Epoch 93/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6201 - acc: 0.8365 - val_loss: 0.6402 - val_acc: 0.8308\n",
      "Epoch 94/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6200 - acc: 0.8363 - val_loss: 0.6401 - val_acc: 0.8308\n",
      "Epoch 95/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6200 - acc: 0.8363 - val_loss: 0.6400 - val_acc: 0.8308\n",
      "Epoch 96/100\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.6198 - acc: 0.8365 - val_loss: 0.6401 - val_acc: 0.8307\n",
      "Epoch 97/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6198 - acc: 0.8365 - val_loss: 0.6400 - val_acc: 0.8305\n",
      "Epoch 98/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6197 - acc: 0.8364 - val_loss: 0.6399 - val_acc: 0.8312\n",
      "Epoch 99/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6196 - acc: 0.8364 - val_loss: 0.6399 - val_acc: 0.8309\n",
      "Epoch 100/100\n",
      "200000/200000 [==============================] - 2s 8us/step - loss: 0.6195 - acc: 0.8364 - val_loss: 0.6399 - val_acc: 0.8307\n",
      "10000/10000 [==============================] - 0s 13us/step\n",
      "\n",
      "Test score: 0.3926052177429199\n",
      "Test accuracy: 0.8988\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=train_dataset, y=train_labels,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_data=(valid_dataset, valid_labels))\n",
    "score = model.evaluate(test_dataset, test_labels, verbose=VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
